# vlm_train_optimized.py - ê·¹í•œì˜ ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „

import os
import sys
import json
import argparse
import torch
from torch.utils.data import DataLoader
from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import gc

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€ (ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ì„ ìœ„í•´)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from src.config.config import CFG, DEVICE, TRAIN_IMG_DIR, PROJECT_ROOT
from src.utils.data.enhanced_dataset import EnhancedImageDataset
from src.utils.data.vlm_dataset import LlavaFineTuningDataset
from src.utils.utils import seed_everything

def clear_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    torch.cuda.empty_cache()
    if torch.cuda.is_available():
        torch.cuda.synchronize()

def get_optimized_model(model_name="llava-hf/llava-1.5-7b-hf"):
    """ê·¹í•œìœ¼ë¡œ ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ"""
    from transformers import AutoProcessor, LlavaForConditionalGeneration
    
    # ë” ê³µê²©ì ì¸ ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16  # bfloat16 ëŒ€ì‹  float16
    )
    
    # CPU offloading í™œì„±í™”
    device_map = {
        "vision_tower": 0,  # GPU
        "multi_modal_projector": 0,  # GPU
        "language_model.model.embed_tokens": 0,  # GPU
        "language_model.model.layers": 0,  # GPUì— í• ë‹¹
        "language_model.model.norm": 0,  # GPU
        "language_model.lm_head": 0,  # GPUë¡œ ë³€ê²½
    }
    
    print("ğŸ”„ Loading model with aggressive optimization...")
    model = LlavaForConditionalGeneration.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map=device_map,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        offload_folder="offload",  # ì˜¤í”„ë¡œë“œ í´ë”
    )
    
    processor = AutoProcessor.from_pretrained(model_name)
    
    # Gradient checkpointing í™œì„±í™”
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)
    
    # ë” ì‘ì€ LoRA ì„¤ì •
    lora_config = LoraConfig(
        r=8,  # 16ì—ì„œ 8ë¡œ ê°ì†Œ
        lora_alpha=16,  # 32ì—ì„œ 16ìœ¼ë¡œ ê°ì†Œ
        target_modules=["q_proj", "v_proj"],  # íƒ€ê²Ÿ ëª¨ë“ˆ ì¤„ì´ê¸°
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model, processor

def train_with_minimal_memory(args):
    """ìµœì†Œ ë©”ëª¨ë¦¬ë¡œ í•™ìŠµ"""
    seed_everything(args.seed)
    clear_memory()
    
    # 1. ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
    print("ğŸ“Š Preparing data...")
    clip_dataset = EnhancedImageDataset(
        data_dir=TRAIN_IMG_DIR,
        processor=None,
        mode='train',
        model_type='clip'
    )
    
    # ì´ë¯¸ì§€ ID ë§¤í•‘ ìƒì„±
    image_id_to_path_map = {}
    image_id_to_label_map = {}
    
    for i in range(len(clip_dataset)):
        info = clip_dataset.get_img_info(i)
        image_id_to_path_map[info['image_id']] = info['abs_path']
        image_id_to_label_map[info['image_id']] = info['label_text']
    
    # 2. ëª¨ë¸ ë¡œë“œ
    model, processor = get_optimized_model(args.model_name)
    clear_memory()
    
    # 3. ë°ì´í„°ì…‹ ìƒì„± (ì‘ì€ ì„œë¸Œì…‹ë§Œ ì‚¬ìš©)
    if args.subset_size > 0:
        # ë°ì´í„° ì„œë¸Œì…‹ë§Œ ì‚¬ìš©
        subset_ids = list(image_id_to_path_map.keys())[:args.subset_size]
        subset_path_map = {k: image_id_to_path_map[k] for k in subset_ids}
        subset_label_map = {k: image_id_to_label_map[k] for k in subset_ids}
    else:
        subset_path_map = image_id_to_path_map
        subset_label_map = image_id_to_label_map
    
    vlm_dataset = LlavaFineTuningDataset(
        image_id_to_path_map=subset_path_map,
        image_id_to_label_map=subset_label_map,
        model_a_top_k_json_path=args.train_top_k_json,
        processor=processor,
        max_length=args.max_length,
        prompt_top_k=args.prompt_top_k
    )
    
    # 4. ìµœì í™”ëœ í•™ìŠµ ì„¤ì •
    from transformers import TrainingArguments, Trainer
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        
        # ê·¹í•œì˜ ë©”ëª¨ë¦¬ ìµœì í™”
        per_device_train_batch_size=1,
        gradient_accumulation_steps=args.grad_accum_steps,
        gradient_checkpointing=True,
        
        # Mixed precision
        fp16=True,
        fp16_full_eval=False,
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optim="adamw_bnb_8bit",  # 8-bit optimizer
        learning_rate=args.learning_rate,
        
        # ì €ì¥ ì„¤ì •
        save_strategy="epoch",
        save_total_limit=1,
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        dataloader_pin_memory=False,
        max_grad_norm=0.3,
        
        # DeepSpeed (ì„ íƒì‚¬í•­)
        # deepspeed="ds_config.json",  # DeepSpeed ì„¤ì • íŒŒì¼
        
        # ê¸°íƒ€
        logging_steps=10,
        report_to="none",
        remove_unused_columns=False,
    )
    
    # Custom collate function
    def collate_fn(batch):
        batch = [item for item in batch if item is not None]
        if not batch:
            return {}
        
        # íŒ¨ë”© ëŒ€ì‹  ê°œë³„ ì²˜ë¦¬
        return {
            "input_ids": torch.stack([item["input_ids"] for item in batch]),
            "attention_mask": torch.stack([item["attention_mask"] for item in batch]),
            "pixel_values": torch.stack([item["pixel_values"] for item in batch]),
            "labels": torch.stack([item["labels"] for item in batch])
        }
    
    # 5. Trainer ìƒì„± ë° í•™ìŠµ
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=vlm_dataset,
        data_collator=collate_fn,
        tokenizer=processor.tokenizer,
    )
    
    print("ğŸš€ Starting optimized training...")
    try:
        # í•™ìŠµ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_memory()
        
        # í•™ìŠµ
        trainer.train()
        
        # ì €ì¥
        trainer.save_model()
        processor.save_pretrained(args.output_dir)
        
        print("âœ… Training completed successfully!")
        
    except torch.cuda.OutOfMemoryError:
        print("âŒ Still OOM! Try these options:")
        print("1. Reduce --max_length (current: {})".format(args.max_length))
        print("2. Increase --grad_accum_steps (current: {})".format(args.grad_accum_steps))
        print("3. Use --subset_size to train on fewer samples")
        print("4. Use a smaller model with --model_name")
        raise

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    
    parser.add_argument('--train_top_k_json', type=str, required=True)
    parser.add_argument('--model_name', type=str, 
                       default="llava-hf/llava-1.5-7b-hf",
                       help="Model to use (try smaller ones if OOM)")
    parser.add_argument('--max_length', type=int, default=768,  # ì¤„ì„
                       help='Maximum sequence length')
    parser.add_argument('--prompt_top_k', type=int, default=3)
    parser.add_argument('--grad_accum_steps', type=int, default=16)  # ëŠ˜ë¦¼
    parser.add_argument('--epochs', type=int, default=1)  # ì¤„ì„
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    parser.add_argument('--subset_size', type=int, default=0,
                       help='Use only N samples (0 = use all)')
    parser.add_argument('--output_dir', type=str, 
                       default='outputs/vlm_optimized')
    parser.add_argument('--seed', type=int, default=42)
    
    args = parser.parse_args()
    train_with_minimal_memory(args)
